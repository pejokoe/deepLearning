{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt19N3SLYmo_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset, random_split\n",
        "import math\n",
        "\n",
        "# temporary\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e266Xe-oDxTT"
      },
      "source": [
        "# Deep Learning project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3s-kJk_DRJl"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRl82uNgDz0X"
      },
      "source": [
        "Clone the 'Hey Waldo' repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no6kL4eRtbMr"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/vc1492a/Hey-Waldo.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsRq46H4DXDx"
      },
      "source": [
        "The first step is to load our images dataset. To do this, we define a transformation that is used on the images. Here we resize all of our images to 128x128. Then, we load the 64x64 and 128x128 images in and apply the transformation on every image. Lastly, we combine the 2 datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn_KpqQ7DauT"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.Resize(128),\n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "images_64 = datasets.ImageFolder('/content/Hey-Waldo/64', transform = transform)\n",
        "images_128 = datasets.ImageFolder('/content/Hey-Waldo/128', transform = transform)\n",
        "images = ConcatDataset([images_64, images_128])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o9ZG7T2MsT9"
      },
      "source": [
        "Next, we split our images into a train, validation and test set. The distribution is 81% of our images are used for training, 9% for validation and the other 10% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtzIWBb6MrOZ"
      },
      "outputs": [],
      "source": [
        "N_train = int(0.9 * len(images))\n",
        "N_test = int(0.1 * len(images))\n",
        "N_validation = int(math.ceil(N_train * 0.1))\n",
        "\n",
        "train, test_set = random_split(images, [N_train, N_test])\n",
        "train_set, validation_set = random_split(train, [int(N_train * 0.9), N_validation])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM-upTBdCzkS"
      },
      "source": [
        "We calculate the mean and standard deviation of the training set so that we can use this to normalize our data. We also define some augmentations for our images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSWe7k3MClD9"
      },
      "outputs": [],
      "source": [
        "def calc_mean_std(data):\n",
        "  images = [item[0] for item in data]\n",
        "  mean = torch.mean(torch.stack(images, dim=0), dim=(0, 2, 3))\n",
        "  std = torch.std(torch.stack(images, dim=0), dim=(0, 2, 3))\n",
        "  return mean, std\n",
        "\n",
        "mean, std = calc_mean_std(train_set)\n",
        "\n",
        "train_transformer = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.Normalize(mean, std)]) # Add more augmentations depending on what we want\n",
        "test_transformer = transforms.Compose([transforms.Normalize(mean, std)])\n",
        "\n",
        "# Apply transformation on train, validation and test sets"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
